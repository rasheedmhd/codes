Recon
1. Confirm that active recon is allowed
2. Manual scope walk through - aim is to reveal the attack surface, all the end points we can launch an attack test
3. Uncover the tech and infra+structure of the application
4. Google dork for info to advance your hacking, eg: searching for payloads

Vuln Search
https://www.cvedetails.com/documentation/

SCOPE DISCOVERY
1. whois and reverse whois
2. nslookup
3. certificates searching with crt.sh
4. Enumerate subdomains
Sublist3r - query search engines + subdomain dbs
Gobuster - brute forcer
Amass - uses DNS zone transfers, certificate parsing, search engines, and subdomain databases to
find subdomains.
Altdns - https://github.com/infosec-au/altdns/
SubBrute - brute forcer
Subfinder

Wordlists
https://github.com/danielmiessler/SecLists/
https://github.com/assetnote/commonspeak2/

5. Service Enumeration (port scanning)
- nmap, masscan
passive reconn
Shodan
Project Sonar
Censys
6. Crawl all pages - ZAP Spider
7. Third Party Hosting 
- Use google dorking to plus tools like lazys3
https://buckets.grayhatwarfare.com/
to discover open buckets, use aws cli to read the bucket contents
https://github.com/nahamsec/lazys3/ - I can rewrite it in Rust/Go/TypeScript
8. Github Recon - tools 
Gitrob (https://github.com/michenriksen/gitrob/)
TruffleHog (https://github.com/trufflesecurity/truffleHog/) 
9. OSINT
10. Fingerprinting - 
- nmap domain_name -sV
- inspecting req headers in burp proxy
- view html sc  
tools - wapplyzer, builtwith, stackshare, retire.js


OWASP WSTG - 4.1.X
1 Conduct Search Engine Discovery Reconnaissance for Information Leakage
- Google dorking
- Use different search engines bc they index differently
- Get robots.txt file
2. Fingerprinting web servers
- banner grab ( curl -I domain)
- server response adds the server name and version
- if server obscures name and version, study response header ordering
Send Malformed Requests
- study default response error msgs
- tools - netcraft, nikto, nmap

3 Review Webserver Metafiles for Information Leakage
Identify hidden or obfuscated paths and functionality through the analysis of metadata files.
Extract and map other information that could lead to a better understanding of the systems at hand
- META tags
- Robots.txt
- Sitemaps
- Security.txt
- humans.txt
curl -O -Ss https://www.google.com/robots.txt && head -n5 robots.txt

3. Enumerate Applications on Webserver
Enumerate the applications within the scope that exist on a web server.
Different base urls - use site: operator
Nonstandard ports - web apps can be hosted on different ports != 443/80
nmap -sV
nmap –Pn –sT –sV –p0-65535
Virtual Hosts - DNS allows a single IP address to be associated with one or more symbolic names.

5 Review Web Page Content for Information Leakage
Review Web Page Comments and Metadata
Identifying JavaScript Code and Gathering JavaScript Files
Identifying Source Map Files
Identify Redirect Responses which Leak Information













